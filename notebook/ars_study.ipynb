{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Simple random search for RL\n",
    "\n",
    "https://arxiv.org/pdf/1803.07055.pdf\n",
    "\n",
    "https://github.com/jietan/ARS\n",
    "\n",
    "using PyBullet:\n",
    "https://github.com/bulletphysics/bullet3/issues/1718\n",
    "    \n",
    "https://github.com/YungKC/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/ARS/ars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1000) is deprecated. Use register(max_episode_steps=1000) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "# Importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Pipe\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Hyper Parameters\n",
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 10000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocess Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "_RESET = 1\n",
    "_CLOSE = 2\n",
    "_EXPLORE = 3\n",
    "\n",
    "def ExploreWorker(rank,childPipe, envname):\n",
    "    env = gym.make(envname)\n",
    "    nb_inputs = env.observation_space.shape[0]\n",
    "    normalizer = Normalizer(nb_inputs)\n",
    "    observation_n = env.reset()\n",
    "    n=0\n",
    "    while True:\n",
    "      n+=1\n",
    "      try:\n",
    "        # Only block for short times to have keyboard exceptions be raised.\n",
    "        if not childPipe.poll(0.001):\n",
    "          continue\n",
    "        message, payload = childPipe.recv()\n",
    "      except (EOFError, KeyboardInterrupt):\n",
    "        break\n",
    "      if message == _RESET:\n",
    "        observation_n = env.reset()\n",
    "        childPipe.send([\"reset ok\"])\n",
    "        continue\n",
    "      if message == _EXPLORE:\n",
    "        #normalizer = payload[0] #use our local normalizer\n",
    "        policy = payload[1]\n",
    "        hp = payload[2]\n",
    "        direction = payload[3]\n",
    "        delta = payload[4]\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.\n",
    "        sum_rewards = 0\n",
    "        while not done and num_plays < hp.episode_length:\n",
    "            normalizer.observe(state)\n",
    "            state = normalizer.normalize(state)\n",
    "            action = policy.evaluate(state, delta, direction,hp)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        childPipe.send([sum_rewards])\n",
    "        continue\n",
    "      if message == _CLOSE:\n",
    "        childPipe.send([\"close ok\"])\n",
    "        break\n",
    "    childPipe.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the states\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    def __init__(self, input_size, output_size, env_name):\n",
    "        try:\n",
    "          self.theta = np.load(POLICY)\n",
    "        except:\n",
    "          self.theta = np.zeros((output_size, input_size))\n",
    "        self.env_name = env_name\n",
    "        print(\"Starting policy theta=\",self.theta)\n",
    "    def evaluate(self, input, delta, direction, hp):\n",
    "        if direction is None:\n",
    "            return np.clip(self.theta.dot(input), -1.0, 1.0)\n",
    "        elif direction == \"positive\":\n",
    "            return np.clip((self.theta + hp.noise*delta).dot(input), -1.0, 1.0)\n",
    "        else:\n",
    "            return np.clip((self.theta - hp.noise*delta).dot(input), -1.0, 1.0)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        np.save(LOGDIR+\"/policy_\"+self.env_name+\"_\"+timestr+\".npy\", self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction, delta, hp):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction, hp)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the AI\n",
    "\n",
    "def train(env, policy, normalizer, hp, parentPipes):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        if parentPipes:\n",
    "          for k in range(hp.nb_directions):\n",
    "            parentPipe = parentPipes[k]\n",
    "            parentPipe.send([_EXPLORE,[normalizer, policy, hp, \"positive\", deltas[k]]])\n",
    "          for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = parentPipes[k].recv()[0]\n",
    "          \n",
    "          for k in range(hp.nb_directions):\n",
    "            parentPipe = parentPipes[k]\n",
    "            parentPipe.send([_EXPLORE,[normalizer, policy, hp, \"negative\", deltas[k]]])\n",
    "          for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = parentPipes[k].recv()[0]\n",
    "          \n",
    "        else:\n",
    "          # Getting the positive rewards in the positive directions\n",
    "          for k in range(hp.nb_directions):\n",
    "              positive_rewards[k] = explore(env, normalizer, policy, \"positive\", deltas[k], hp)\n",
    "        \n",
    "          \n",
    "          # Getting the negative rewards in the negative/opposite directions\n",
    "          for k in range(hp.nb_directions):\n",
    "              negative_rewards[k] = explore(env, normalizer, policy, \"negative\", deltas[k], hp)\n",
    "            \n",
    "        \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x])[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy, None, None, hp)\n",
    "        print('Step:', step, 'Reward:', reward_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the main code\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed =  1\n",
      "Starting policy theta= [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "start training\n",
      "Step: 0 Reward: -950.3551718397453\n",
      "Step: 1 Reward: -952.5140566628097\n",
      "Step: 2 Reward: -959.9700043118507\n",
      "Step: 3 Reward: -959.8624185844203\n",
      "Step: 4 Reward: -967.8364906812106\n",
      "Step: 5 Reward: -966.2553082736092\n",
      "Step: 6 Reward: -934.4684694716061\n",
      "Step: 7 Reward: -971.6289875050481\n",
      "Step: 8 Reward: -274.519013812337\n",
      "Step: 9 Reward: -824.8789039137075\n",
      "Step: 10 Reward: -922.9668753088652\n",
      "Step: 11 Reward: 365.1661095773217\n",
      "Step: 12 Reward: -826.3627455211911\n",
      "Step: 13 Reward: -925.0431479959245\n",
      "Step: 14 Reward: -911.9464267993249\n",
      "Step: 15 Reward: -922.997448558917\n",
      "Step: 16 Reward: 432.9296170645419\n",
      "Step: 17 Reward: 285.3015631842454\n",
      "Step: 18 Reward: 437.51284082774777\n",
      "Step: 19 Reward: -935.1514521885364\n",
      "Step: 20 Reward: -791.4845451859956\n",
      "Step: 21 Reward: -550.105181019647\n",
      "Step: 22 Reward: -627.8976340951925\n",
      "Step: 23 Reward: -910.927090390321\n",
      "Step: 24 Reward: -822.0845294182216\n",
      "Step: 25 Reward: -450.9555370924106\n",
      "Step: 26 Reward: -747.4733243635425\n",
      "Step: 27 Reward: -162.6225369366039\n",
      "Step: 28 Reward: -264.4953424770445\n",
      "Step: 29 Reward: -352.7407686831884\n",
      "Step: 30 Reward: -258.898862473484\n",
      "Step: 31 Reward: -342.9475834797008\n",
      "Step: 32 Reward: -255.96408010168454\n",
      "Step: 33 Reward: -56.472464870594195\n",
      "Step: 34 Reward: -121.2986735069441\n",
      "Step: 35 Reward: -48.503868964499745\n",
      "Step: 36 Reward: -116.20936759861709\n",
      "Step: 37 Reward: -129.28609992119127\n",
      "Step: 38 Reward: -152.5366991915956\n",
      "Step: 39 Reward: -116.11992791503171\n",
      "Step: 40 Reward: -88.75417452868383\n",
      "Step: 41 Reward: 35.69645057697555\n",
      "Step: 42 Reward: 37.61487296175589\n",
      "Step: 43 Reward: -34.33108381113053\n",
      "Step: 44 Reward: -83.53025415645114\n",
      "Step: 45 Reward: 9.152140451685542\n",
      "Step: 46 Reward: -47.915437134234374\n",
      "Step: 47 Reward: -21.385493346200754\n",
      "Step: 48 Reward: -48.57698646518984\n",
      "Step: 49 Reward: -32.77948802444047\n",
      "Step: 50 Reward: 134.63839163184002\n",
      "Step: 51 Reward: 63.56310728460816\n",
      "Step: 52 Reward: 122.30416585091783\n",
      "Step: 53 Reward: 53.3364024970039\n",
      "Step: 54 Reward: 169.19391386080076\n",
      "Step: 55 Reward: 189.70470639849125\n",
      "Step: 56 Reward: 104.66945858830762\n",
      "Step: 57 Reward: 172.24161297410728\n",
      "Step: 58 Reward: 170.05169432264861\n",
      "Step: 59 Reward: 138.23295250917263\n",
      "Step: 60 Reward: 139.83840191733893\n",
      "Step: 61 Reward: 118.94716873634968\n",
      "Step: 62 Reward: 115.70058412599971\n",
      "Step: 63 Reward: 195.26584463992072\n",
      "Step: 64 Reward: 256.37212005824813\n",
      "Step: 65 Reward: 148.22570192957124\n",
      "Step: 66 Reward: 322.1905408971448\n",
      "Step: 67 Reward: 305.3419113665876\n",
      "Step: 68 Reward: 360.0149306024814\n",
      "Step: 69 Reward: 365.31510664913435\n",
      "Step: 70 Reward: 254.04849024628194\n",
      "Step: 71 Reward: 445.06317690292076\n",
      "Step: 72 Reward: 352.1334915175941\n",
      "Step: 73 Reward: 361.20507111098954\n",
      "Step: 74 Reward: 417.9043551197949\n",
      "Step: 75 Reward: 415.66241192477725\n",
      "Step: 76 Reward: 381.1186936185001\n",
      "Step: 77 Reward: 332.0678254792632\n",
      "Step: 78 Reward: 387.75275596627444\n",
      "Step: 79 Reward: 340.2072466644061\n",
      "Step: 80 Reward: 348.8644810968974\n",
      "Step: 81 Reward: 361.9273874126507\n",
      "Step: 82 Reward: 343.7444486376792\n",
      "Step: 83 Reward: 419.50930312926283\n",
      "Step: 84 Reward: 440.3989761048554\n",
      "Step: 85 Reward: 379.98804652313027\n",
      "Step: 86 Reward: 374.1469954463365\n",
      "Step: 87 Reward: 403.679965296704\n",
      "Step: 88 Reward: 497.2190367217955\n",
      "Step: 89 Reward: 397.3172859943945\n",
      "Step: 90 Reward: 528.4500117635534\n",
      "Step: 91 Reward: 516.9220563562998\n",
      "Step: 92 Reward: 476.9735324106627\n",
      "Step: 93 Reward: 466.35629249205806\n",
      "Step: 94 Reward: 470.6071208877756\n",
      "Step: 95 Reward: 505.30322444364873\n",
      "Step: 96 Reward: 412.50898674610744\n",
      "Step: 97 Reward: 537.2872476597968\n",
      "Step: 98 Reward: 456.2918151413725\n"
     ]
    }
   ],
   "source": [
    "mp.freeze_support()\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--env', help='Gym environment name', type=str, default='HalfCheetahBulletEnv-v0')\n",
    "parser.add_argument('--seed', help='RNG seed', type=int, default=1)\n",
    "parser.add_argument('--render', help='OpenGL Visualizer', type=int, default=0)\n",
    "parser.add_argument('--movie',help='rgb_array gym movie',type=int, default=0)\n",
    "parser.add_argument('--steps', help='Number of steps', type=int, default=10000)\n",
    "parser.add_argument('--policy', help='Starting policy file (npy)', type=str, default='')\n",
    "parser.add_argument('--logdir', help='Directory root to log policy files (npy)', type=str, default='.')\n",
    "parser.add_argument('--mp', help='Enable multiprocessing', type=int, default=1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "MULTI_PROCESSING = 1\n",
    "RENDER = 0\n",
    "MOVIE = 0\n",
    "POLICY = ''\n",
    "LOGDIR = '.'\n",
    "STEPS = 10000\n",
    "\n",
    "\n",
    "hp = Hp()\n",
    "hp.env_name = 'HalfCheetahBulletEnv-v0'\n",
    "hp.seed = 1\n",
    "hp.nb_steps = 10000\n",
    "print(\"seed = \", hp.seed)\n",
    "np.random.seed(hp.seed)\n",
    "\n",
    "parentPipes = None\n",
    "if MULTI_PROCESSING:\n",
    "  num_processes = hp.nb_directions\n",
    "  processes = []\n",
    "  childPipes = []\n",
    "  parentPipes = []\n",
    "\n",
    "  for pr in range (num_processes):\n",
    "    parentPipe, childPipe = Pipe()\n",
    "    parentPipes.append(parentPipe)\n",
    "    childPipes.append(childPipe)\n",
    "\n",
    "  for rank in range(num_processes):\n",
    "      p = mp.Process(target=ExploreWorker, args=(rank,childPipes[rank], hp.env_name))\n",
    "      p.start()\n",
    "      processes.append(p)\n",
    "\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "env = gym.make(hp.env_name)\n",
    "if RENDER:\n",
    "  env.render(mode = \"human\")\n",
    "if MOVIE:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs,hp.env_name)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "\n",
    "print(\"start training\")\n",
    "train(env, policy, normalizer, hp, parentPipes)\n",
    "\n",
    "if MULTI_PROCESSING:\n",
    "  for parentPipe in parentPipes:\n",
    "    parentPipe.send([_CLOSE,\"pay2\"])\n",
    "\n",
    "  for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
