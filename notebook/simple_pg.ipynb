{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study simple gradient policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network.\n",
    "    for size in sizes[:-1]:\n",
    "        x = tf.layers.dense(x, units=size, activation=activation)\n",
    "    return tf.layers.dense(x, units=sizes[-1], activation=output_activation)\n",
    "\n",
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=5, batch_size=5000, render=False):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "    logits = mlp(obs_ph, sizes=hidden_sizes+[n_acts])\n",
    "    print(f'logits shape: {logits.shape}')\n",
    "    \n",
    "    tmp = tf.multinomial(logits=logits,num_samples=1)\n",
    "    print(f'multinomial shape: {tmp.shape}')\n",
    "    \n",
    "    # make action selection op (outputs int actions, sampled from policy)\n",
    "    actions = tf.squeeze(tmp, axis=1)\n",
    "    print(f'actons shape: {actions.shape}')\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    action_masks = tf.one_hot(act_ph, n_acts)\n",
    "    log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
    "    loss = -tf.reduce_mean(weights_ph * log_probs)\n",
    "\n",
    "    # make train op\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        batch_loss, _ = sess.run([loss, train_op],\n",
    "                                 feed_dict={\n",
    "                                    obs_ph: np.array(batch_obs),\n",
    "                                    act_ph: np.array(batch_acts),\n",
    "                                    weights_ph: np.array(batch_weights)\n",
    "                                 })\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env_name='CartPole-v0', render=False, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_probab = np.array([[1/9.,1/4.,7/3.]])\n",
    "print(in_probab.shape)\n",
    "samples = tf.multinomial(logits=tf.log(in_probab), num_samples=1)\n",
    "tmp = isess.run(tf.squeeze(samples, axis=1))\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0, 0, 0]\n",
    "for _ in range(1000):\n",
    "    pick = isess.run(tf.squeeze(samples, axis=1))[0]\n",
    "    count[pick] += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [32]+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Core Algorithm in Tensorflow\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "    logits = mlp(obs_ph, sizes=hidden_sizes+[n_acts])\n",
    "    print(f'logits shape: {logits.shape}')\n",
    "    \n",
    "    tmp = tf.multinomial(logits=logits,num_samples=1)\n",
    "    print(f'multinomial shape: {tmp.shape}')\n",
    "    \n",
    "    # make action selection op (outputs int actions, sampled from policy)\n",
    "    actions = tf.squeeze(tmp, axis=1)\n",
    "    print(f'actons shape: {actions.shape}')\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    action_masks = tf.one_hot(act_ph, n_acts)\n",
    "    log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
    "    loss = -tf.reduce_mean(weights_ph * log_probs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "logit == log of odds\n",
    "\n",
    "for binary classification: logit(p) = log of (p / (1-p))\n",
    "\n",
    "for multiple classes, then it's called cross entropy\n",
    "\n",
    "e.g. if 3 choices, with probability of 0.1, 0.2, 0.7\n",
    "then \n",
    "logit[0] = ln(0.1)\n",
    "logit[1] = ln(0.2)\n",
    "logit[2] = ln(0.7)\n",
    "\n",
    "multinomial == the random sampling of choice based on logit\n",
    "returns 0, 1 or 2, with odds of 1:2:7\n",
    "        \n",
    "one-shot == given choice ID, return a 1 encoded array\n",
    "e.g., one-shot(3, 5) = [0,0,0,1,0]\n",
    "\n",
    "softmax(xi) == e^xi / Sum(e^xi)\n",
    "is a normalized probability of an array of values\n",
    "vi is any value\n",
    "0 <= s(vi) <=1\n",
    "\n",
    "-log(softmax) == negative log likihood\n",
    "models how bad the selection to the actual answer\n",
    "ref: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "        \n",
    "\n",
    "Vililla Gradient Policy Algorithm:\n",
    "loss = G * log_probability\n",
    "\n",
    "where G == (un)discounted reward value over the episode\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()\n",
    "in_probab = np.array([[0.1, 0.2, 0.7]])\n",
    "print(in_probab.shape)\n",
    "samples = tf.multinomial(logits=tf.log(in_probab), num_samples=10000)\n",
    "tmp = isess.run(tf.squeeze(samples))\n",
    "isess.close()\n",
    "\n",
    "print(tmp.shape)\n",
    "print(tmp)\n",
    "\n",
    "from collections import Counter\n",
    "Counter(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work through exercise 2 solutions\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.arange(5))\n",
    "y = tf.constant(np.arange(5).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(x)\n",
    "sess.run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1000) is deprecated. Use register(max_episode_steps=1000) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9569a452f473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'baselines'"
     ]
    }
   ],
   "source": [
    "import baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedDoublePendulumBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybullet.connect(pybullet.DIRECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinup.ddpg(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetahBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
