{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN study\n",
    "ref: https://github.com/jaromiru/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "\n",
    "https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenGym Seaquest-v0\n",
    "# -------------------\n",
    "#\n",
    "# This code demonstrates a Double DQN network with Priority Experience Replay\n",
    "# in an OpenGym Seaquest-v0 environment.\n",
    "#\n",
    "# Made as part of blog series Let's make a DQN, available at: \n",
    "# https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "# \n",
    "# author: Jaromir Janisch, 2016\n",
    "\n",
    "import random, numpy, math, gym, scipy\n",
    "\n",
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "IMAGE_STACK = 2\n",
    "\n",
    "HUBER_LOSS_DELTA = 2.0\n",
    "LEARNING_RATE = 0.00025\n",
    "\n",
    "#-------------------- UTILITIES -----------------------\n",
    "def huber_loss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "\n",
    "    cond = K.abs(err) < HUBER_LOSS_DELTA\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = HUBER_LOSS_DELTA * (K.abs(err) - 0.5 * HUBER_LOSS_DELTA)\n",
    "\n",
    "    loss = tf.where(cond, L2, L1)   # Keras does not cover where function in tensorflow :-(\n",
    "\n",
    "    return K.mean(loss)\n",
    "\n",
    "def processImage( img ):\n",
    "    rgb = scipy.misc.imresize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), interp='bilinear')\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b     # extract luminance\n",
    "\n",
    "    o = gray.astype('float32') / 128 - 1    # normalize\n",
    "    return o\n",
    "\n",
    "#-------------------- BRAIN ---------------------------\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        self.model_ = self._createModel()  # target network\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', input_shape=(self.stateCnt), data_format='channels_first'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "        model.add(Dense(units=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=LEARNING_RATE)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT), target).flatten()\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "\n",
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "#-------------------- AGENT ---------------------------\n",
    "MEMORY_CAPACITY = 200000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "EXPLORATION_STOP = 500000   # at this step epsilon will be 0.01\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STOP  # speed of decay\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        # self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return numpy.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        x, y, errors = self._getTargets([(0, sample)])\n",
    "        self.memory.add(errors[0], sample)\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def _getTargets(self, batch):\n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "\n",
    "        states = numpy.array([ o[1][0] for o in batch ])\n",
    "        states_ = numpy.array([ (no_state if o[1][3] is None else o[1][3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "\n",
    "        p_ = agent.brain.predict(states_, target=False)\n",
    "        pTarget_ = agent.brain.predict(states_, target=True)\n",
    "\n",
    "        x = numpy.zeros((len(batch), IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        y = numpy.zeros((len(batch), self.actionCnt))\n",
    "        errors = numpy.zeros(len(batch))\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            oldVal = t[a]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * pTarget_[i][ numpy.argmax(p_[i]) ]  # double DQN\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        x, y, errors = self._getTargets(batch)\n",
    "\n",
    "        #update errors\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "\n",
    "class RandomAgent:\n",
    "    memory = Memory(MEMORY_CAPACITY)\n",
    "    exp = 0\n",
    "\n",
    "    def __init__(self, actionCnt):\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "    def act(self, s):\n",
    "        return random.randint(0, self.actionCnt-1)\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        error = abs(sample[2])  # reward\n",
    "        self.memory.add(error, sample)\n",
    "        self.exp += 1\n",
    "\n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "#-------------------- ENVIRONMENT ---------------------\n",
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "    def run(self, agent):                \n",
    "        img = self.env.reset()\n",
    "        w = processImage(img)\n",
    "        s = numpy.array([w, w])\n",
    "\n",
    "        R = 0\n",
    "        while True:         \n",
    "            # self.env.render()\n",
    "            a = agent.act(s)\n",
    "\n",
    "            r = 0\n",
    "            img, r, done, info = self.env.step(a)\n",
    "            s_ = numpy.array([s[1], processImage(img)]) #last two screens\n",
    "\n",
    "            r = np.clip(r, -1, 1)   # clip reward to [-1, 1]\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Total reward:\", R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM = 'Seaquest-v0'\n",
    "env = Environment(PROBLEM)\n",
    "\n",
    "stateCnt  = (IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "randomAgent = RandomAgent(actionCnt)\n",
    "\n",
    "try:\n",
    "    print(\"Initialization with random agent...\")\n",
    "    while randomAgent.exp < MEMORY_CAPACITY:\n",
    "        env.run(randomAgent)\n",
    "        print(randomAgent.exp, \"/\", MEMORY_CAPACITY)\n",
    "\n",
    "    agent.memory = randomAgent.memory\n",
    "\n",
    "    randomAgent = None\n",
    "\n",
    "    print(\"Starting learning\")\n",
    "    while True:\n",
    "        env.run(agent)\n",
    "finally:\n",
    "    agent.brain.model.save(\"Seaquest-DQN-PER.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
