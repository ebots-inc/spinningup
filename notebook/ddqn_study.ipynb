{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN study\n",
    "ref: https://github.com/jaromiru/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "\n",
    "https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# OpenGym Seaquest-v0\n",
    "# -------------------\n",
    "#\n",
    "# This code demonstrates a Double DQN network with Priority Experience Replay\n",
    "# in an OpenGym Seaquest-v0 environment.\n",
    "#\n",
    "# Made as part of blog series Let's make a DQN, available at: \n",
    "# https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "# \n",
    "# author: Jaromir Janisch, 2016\n",
    "\n",
    "import random, numpy, math, gym, scipy\n",
    "\n",
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "IMAGE_STACK = 2\n",
    "\n",
    "HUBER_LOSS_DELTA = 2.0\n",
    "LEARNING_RATE = 0.00025\n",
    "\n",
    "#-------------------- UTILITIES -----------------------\n",
    "def huber_loss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "\n",
    "    cond = K.abs(err) < HUBER_LOSS_DELTA\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = HUBER_LOSS_DELTA * (K.abs(err) - 0.5 * HUBER_LOSS_DELTA)\n",
    "\n",
    "    loss = tf.where(cond, L2, L1)   # Keras does not cover where function in tensorflow :-(\n",
    "\n",
    "    return K.mean(loss)\n",
    "\n",
    "def processImage( img ):\n",
    "    rgb = scipy.misc.imresize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), interp='bilinear')\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b     # extract luminance\n",
    "\n",
    "    o = gray.astype('float32') / 128 - 1    # normalize\n",
    "    return o\n",
    "\n",
    "#-------------------- BRAIN ---------------------------\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        self.model_ = self._createModel()  # target network\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', input_shape=(self.stateCnt), data_format='channels_first'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "        model.add(Dense(units=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=LEARNING_RATE)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT), target).flatten()\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "\n",
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "#-------------------- AGENT ---------------------------\n",
    "MEMORY_CAPACITY = 100000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "EXPLORATION_STOP = 500000   # at this step epsilon will be 0.01\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STOP  # speed of decay\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        # self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return numpy.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        x, y, errors = self._getTargets([(0, sample)])\n",
    "        self.memory.add(errors[0], sample)\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def _getTargets(self, batch):\n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "\n",
    "        states = numpy.array([ o[1][0] for o in batch ])\n",
    "        states_ = numpy.array([ (no_state if o[1][3] is None else o[1][3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "\n",
    "        p_ = agent.brain.predict(states_, target=False)\n",
    "        pTarget_ = agent.brain.predict(states_, target=True)\n",
    "\n",
    "        x = numpy.zeros((len(batch), IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        y = numpy.zeros((len(batch), self.actionCnt))\n",
    "        errors = numpy.zeros(len(batch))\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            oldVal = t[a]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * pTarget_[i][ numpy.argmax(p_[i]) ]  # double DQN\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        x, y, errors = self._getTargets(batch)\n",
    "\n",
    "        #update errors\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "\n",
    "class RandomAgent:\n",
    "    memory = Memory(MEMORY_CAPACITY)\n",
    "    exp = 0\n",
    "\n",
    "    def __init__(self, actionCnt):\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "    def act(self, s):\n",
    "        return random.randint(0, self.actionCnt-1)\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        error = abs(sample[2])  # reward\n",
    "        self.memory.add(error, sample)\n",
    "        self.exp += 1\n",
    "\n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "#-------------------- ENVIRONMENT ---------------------\n",
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "    def run(self, agent):                \n",
    "        img = self.env.reset()\n",
    "        w = processImage(img)\n",
    "        s = numpy.array([w, w])\n",
    "\n",
    "        R = 0\n",
    "        while True:         \n",
    "            # self.env.render()\n",
    "            a = agent.act(s)\n",
    "\n",
    "            r = 0\n",
    "            img, r, done, info = self.env.step(a)\n",
    "            s_ = numpy.array([s[1], processImage(img)]) #last two screens\n",
    "\n",
    "            r = np.clip(r, -1, 1)   # clip reward to [-1, 1]\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Total reward:\", R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Initialization with random agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 3.0\n",
      "695 / 100000\n",
      "Total reward: 4.0\n",
      "1307 / 100000\n",
      "Total reward: 8.0\n",
      "2206 / 100000\n",
      "Total reward: 4.0\n",
      "2838 / 100000\n",
      "Total reward: 5.0\n",
      "3778 / 100000\n",
      "Total reward: 5.0\n",
      "4494 / 100000\n",
      "Total reward: 7.0\n",
      "5481 / 100000\n",
      "Total reward: 1.0\n",
      "5949 / 100000\n",
      "Total reward: 0.0\n",
      "6359 / 100000\n",
      "Total reward: 5.0\n",
      "7053 / 100000\n",
      "Total reward: 0.0\n",
      "7530 / 100000\n",
      "Total reward: 3.0\n",
      "8133 / 100000\n",
      "Total reward: 5.0\n",
      "8821 / 100000\n",
      "Total reward: 8.0\n",
      "10011 / 100000\n",
      "Total reward: 7.0\n",
      "10993 / 100000\n",
      "Total reward: 9.0\n",
      "12129 / 100000\n",
      "Total reward: 4.0\n",
      "12799 / 100000\n",
      "Total reward: 4.0\n",
      "13467 / 100000\n",
      "Total reward: 10.0\n",
      "14460 / 100000\n",
      "Total reward: 10.0\n",
      "15377 / 100000\n",
      "Total reward: 5.0\n",
      "16184 / 100000\n",
      "Total reward: 2.0\n",
      "16839 / 100000\n",
      "Total reward: 2.0\n",
      "17630 / 100000\n",
      "Total reward: 0.0\n",
      "18130 / 100000\n",
      "Total reward: 6.0\n",
      "18863 / 100000\n",
      "Total reward: 11.0\n",
      "19859 / 100000\n",
      "Total reward: 1.0\n",
      "20329 / 100000\n",
      "Total reward: 0.0\n",
      "20735 / 100000\n",
      "Total reward: 4.0\n",
      "21438 / 100000\n",
      "Total reward: 0.0\n",
      "21830 / 100000\n",
      "Total reward: 7.0\n",
      "22748 / 100000\n",
      "Total reward: 0.0\n",
      "23207 / 100000\n",
      "Total reward: 2.0\n",
      "23850 / 100000\n",
      "Total reward: 4.0\n",
      "24573 / 100000\n",
      "Total reward: 1.0\n",
      "25051 / 100000\n",
      "Total reward: 6.0\n",
      "25872 / 100000\n",
      "Total reward: 1.0\n",
      "26477 / 100000\n",
      "Total reward: 3.0\n",
      "27188 / 100000\n",
      "Total reward: 1.0\n",
      "27717 / 100000\n",
      "Total reward: 3.0\n",
      "28332 / 100000\n",
      "Total reward: 5.0\n",
      "29096 / 100000\n",
      "Total reward: 8.0\n",
      "29942 / 100000\n",
      "Total reward: 8.0\n",
      "30742 / 100000\n",
      "Total reward: 2.0\n",
      "31376 / 100000\n",
      "Total reward: 1.0\n",
      "31971 / 100000\n",
      "Total reward: 3.0\n",
      "32571 / 100000\n",
      "Total reward: 0.0\n",
      "32971 / 100000\n",
      "Total reward: 7.0\n",
      "33878 / 100000\n",
      "Total reward: 4.0\n",
      "34552 / 100000\n",
      "Total reward: 2.0\n",
      "35134 / 100000\n",
      "Total reward: 0.0\n",
      "35526 / 100000\n",
      "Total reward: 7.0\n",
      "36427 / 100000\n",
      "Total reward: 2.0\n",
      "36984 / 100000\n",
      "Total reward: 1.0\n",
      "37520 / 100000\n",
      "Total reward: 3.0\n",
      "38196 / 100000\n",
      "Total reward: 1.0\n",
      "38718 / 100000\n",
      "Total reward: 2.0\n",
      "39403 / 100000\n",
      "Total reward: 1.0\n",
      "39948 / 100000\n",
      "Total reward: 2.0\n",
      "40522 / 100000\n",
      "Total reward: 2.0\n",
      "41189 / 100000\n",
      "Total reward: 0.0\n",
      "41612 / 100000\n",
      "Total reward: 5.0\n",
      "42402 / 100000\n",
      "Total reward: 2.0\n",
      "42981 / 100000\n",
      "Total reward: 2.0\n",
      "43558 / 100000\n",
      "Total reward: 2.0\n",
      "44044 / 100000\n",
      "Total reward: 1.0\n",
      "44566 / 100000\n",
      "Total reward: 5.0\n",
      "45295 / 100000\n",
      "Total reward: 1.0\n",
      "45795 / 100000\n",
      "Total reward: 13.0\n",
      "47244 / 100000\n",
      "Total reward: 3.0\n",
      "47840 / 100000\n",
      "Total reward: 6.0\n",
      "48660 / 100000\n",
      "Total reward: 2.0\n",
      "49230 / 100000\n",
      "Total reward: 5.0\n",
      "50214 / 100000\n",
      "Total reward: 3.0\n",
      "50978 / 100000\n",
      "Total reward: 2.0\n",
      "51591 / 100000\n",
      "Total reward: 4.0\n",
      "52197 / 100000\n",
      "Total reward: 2.0\n",
      "52677 / 100000\n",
      "Total reward: 13.0\n",
      "54323 / 100000\n",
      "Total reward: 0.0\n",
      "54757 / 100000\n",
      "Total reward: 1.0\n",
      "55294 / 100000\n",
      "Total reward: 2.0\n",
      "55819 / 100000\n",
      "Total reward: 2.0\n",
      "56361 / 100000\n",
      "Total reward: 1.0\n",
      "56858 / 100000\n",
      "Total reward: 0.0\n",
      "57295 / 100000\n",
      "Total reward: 2.0\n",
      "57835 / 100000\n",
      "Total reward: 1.0\n",
      "58412 / 100000\n",
      "Total reward: 3.0\n",
      "59396 / 100000\n",
      "Total reward: 3.0\n",
      "60038 / 100000\n",
      "Total reward: 4.0\n",
      "60681 / 100000\n",
      "Total reward: 4.0\n",
      "61298 / 100000\n",
      "Total reward: 4.0\n",
      "61981 / 100000\n",
      "Total reward: 0.0\n",
      "62406 / 100000\n",
      "Total reward: 4.0\n",
      "63106 / 100000\n",
      "Total reward: 9.0\n",
      "64072 / 100000\n",
      "Total reward: 1.0\n",
      "64667 / 100000\n",
      "Total reward: 5.0\n",
      "65414 / 100000\n",
      "Total reward: 0.0\n",
      "65872 / 100000\n",
      "Total reward: 2.0\n",
      "66496 / 100000\n",
      "Total reward: 0.0\n",
      "67021 / 100000\n",
      "Total reward: 3.0\n",
      "67581 / 100000\n",
      "Total reward: 3.0\n",
      "68254 / 100000\n",
      "Total reward: 8.0\n",
      "69184 / 100000\n",
      "Total reward: 1.0\n",
      "69708 / 100000\n",
      "Total reward: 10.0\n",
      "70766 / 100000\n",
      "Total reward: 1.0\n",
      "71289 / 100000\n",
      "Total reward: 2.0\n",
      "71972 / 100000\n",
      "Total reward: 14.0\n",
      "73290 / 100000\n",
      "Total reward: 8.0\n",
      "74218 / 100000\n",
      "Total reward: 5.0\n",
      "75056 / 100000\n",
      "Total reward: 3.0\n",
      "75671 / 100000\n",
      "Total reward: 4.0\n",
      "76384 / 100000\n",
      "Total reward: 2.0\n",
      "76930 / 100000\n",
      "Total reward: 2.0\n",
      "77511 / 100000\n",
      "Total reward: 0.0\n",
      "77939 / 100000\n",
      "Total reward: 11.0\n",
      "78939 / 100000\n",
      "Total reward: 0.0\n",
      "79487 / 100000\n",
      "Total reward: 0.0\n",
      "79918 / 100000\n",
      "Total reward: 7.0\n",
      "80839 / 100000\n",
      "Total reward: 8.0\n",
      "81726 / 100000\n",
      "Total reward: 4.0\n",
      "82379 / 100000\n",
      "Total reward: 4.0\n",
      "83110 / 100000\n",
      "Total reward: 4.0\n",
      "83693 / 100000\n",
      "Total reward: 8.0\n",
      "84621 / 100000\n",
      "Total reward: 5.0\n",
      "85418 / 100000\n",
      "Total reward: 9.0\n",
      "86426 / 100000\n",
      "Total reward: 12.0\n",
      "87868 / 100000\n",
      "Total reward: 7.0\n",
      "88905 / 100000\n",
      "Total reward: 3.0\n",
      "89514 / 100000\n",
      "Total reward: 4.0\n",
      "90205 / 100000\n",
      "Total reward: 4.0\n",
      "90953 / 100000\n",
      "Total reward: 3.0\n",
      "91594 / 100000\n",
      "Total reward: 2.0\n",
      "92203 / 100000\n",
      "Total reward: 4.0\n",
      "92955 / 100000\n",
      "Total reward: 6.0\n",
      "93686 / 100000\n",
      "Total reward: 7.0\n",
      "94709 / 100000\n",
      "Total reward: 16.0\n",
      "96114 / 100000\n",
      "Total reward: 10.0\n",
      "97325 / 100000\n",
      "Total reward: 4.0\n",
      "98125 / 100000\n",
      "Total reward: 4.0\n",
      "98901 / 100000\n",
      "Total reward: 3.0\n",
      "99574 / 100000\n",
      "Total reward: 5.0\n",
      "100364 / 100000\n",
      "Starting learning\n",
      "Total reward: 3.0\n",
      "Total reward: 1.0\n",
      "Total reward: 8.0\n",
      "Total reward: 8.0\n",
      "Total reward: 7.0\n",
      "Total reward: 4.0\n",
      "Total reward: 4.0\n",
      "Total reward: 8.0\n",
      "Total reward: 7.0\n",
      "Total reward: 1.0\n",
      "Total reward: 1.0\n",
      "Total reward: 4.0\n",
      "Total reward: 0.0\n",
      "Total reward: 3.0\n",
      "Total reward: 7.0\n",
      "Total reward: 15.0\n",
      "Total reward: 3.0\n",
      "Total reward: 10.0\n",
      "Total reward: 2.0\n",
      "Total reward: 8.0\n",
      "Total reward: 0.0\n",
      "Total reward: 9.0\n",
      "Total reward: 15.0\n",
      "Total reward: 4.0\n",
      "Total reward: 12.0\n",
      "Total reward: 5.0\n",
      "Total reward: 14.0\n",
      "Total reward: 6.0\n",
      "Total reward: 2.0\n",
      "Total reward: 7.0\n",
      "Total reward: 7.0\n",
      "Total reward: 4.0\n",
      "Total reward: 13.0\n",
      "Total reward: 11.0\n",
      "Total reward: 5.0\n",
      "Total reward: 17.0\n",
      "Total reward: 11.0\n",
      "Total reward: 8.0\n",
      "Total reward: 11.0\n",
      "Total reward: 6.0\n",
      "Total reward: 7.0\n",
      "Total reward: 6.0\n",
      "Total reward: 17.0\n",
      "Total reward: 19.0\n",
      "Total reward: 15.0\n",
      "Total reward: 17.0\n",
      "Total reward: 12.0\n",
      "Total reward: 14.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 12.0\n",
      "Total reward: 9.0\n",
      "Total reward: 15.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 3.0\n",
      "Total reward: 8.0\n",
      "Total reward: 16.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-191b79531ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting learning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Seaquest-DQN-PER.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-525669c4993c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-525669c4993c>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-525669c4993c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, verbose)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PROBLEM = 'Seaquest-v0'\n",
    "env = Environment(PROBLEM)\n",
    "\n",
    "stateCnt  = (IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "randomAgent = RandomAgent(actionCnt)\n",
    "\n",
    "try:\n",
    "    print(\"Initialization with random agent...\")\n",
    "    while randomAgent.exp < MEMORY_CAPACITY:\n",
    "        env.run(randomAgent)\n",
    "        print(randomAgent.exp, \"/\", MEMORY_CAPACITY)\n",
    "\n",
    "    agent.memory = randomAgent.memory\n",
    "\n",
    "    randomAgent = None\n",
    "\n",
    "    print(\"Starting learning\")\n",
    "    while True:\n",
    "        env.run(agent)\n",
    "finally:\n",
    "    agent.brain.model.save(\"Seaquest-DQN-PER.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
